# 我是2.1的标题
我是2.1的正文

| 时间 | 发展 | 描述 |
|------|------|------|
| 早期 | 循环神经网络（RNNs）<br><br>长短期记忆网络（LSTMs）| RNNs能够处理变长的序列数据，但由于梯度消失和梯度爆炸问题，它们在长序列上的表现并不理想。<br> <br>LSTMs是对RNNs的改进，通过引入门控机制来解决长序列学习中的梯度消失问题。|
| 2014年 | Seq2Seq模型的提出<br>[链接](https://arxiv.org/pdf/1409.3215)  | Seq2Seq模型由一个编码器和一个解码器组成，两者都是基于LSTM。该模型在机器翻译任务上取得了显著的成功。<br><br>**历史地位**:<br>  - 首次引入了编码器-解码器框架;<br>- 端到端的学习成为可能;<br>- 中间产生了上下文向量，把编码过程和解码过程进行了解耦。 |
| 2015年 | 注意力机制的引入 | 在Seq2Seq模型的基础上，引入了注意力机制，允许解码器在生成每个元素时关注输入序列的不同部分。 |
| 2017年 | 自注意力与Transformer模型 | Transformer模型完全基于自注意力机制，摒弃了传统的循环网络结构，在处理长距离依赖方面表现卓越。 |
| 2018年 | 多头注意力与BERT | Transformer模型进一步发展为多头注意力机制，并在BERT模型中得到应用，BERT在多项NLP任务上取得了突破性的成果。 |
