# fun-transformer
本课程完全从零开始，使用基础的Numpy等科学计算库实现Transformer，无需依赖任何深度学习框架，旨在深化学习者对模型本质的理解与掌握。
课程涵盖了Transformer的核心组成部分，包含Transformer编码器和解码器的实现。
课程通过解读代码，以及对知识点穿插Q&A问答，为学习者应用Transformer模型提供参考和借鉴。
最后，使用Transformer模型实现在NLP任务中的应用，加深对模型的理解



## 课程大纲
| 章节  | 内容 | 备注|
| ------------- | ------------- |------------- |
| 第一章 | Transformer模型背景 |序列到序列(Seq2Seq)模型的发展；RNN与LSTM的局限性；Transformer的提出与影响 |
| 第二章 | Encoder结构   ||
|第三章   |Decoder结构||
|第四章   | Transformer的训练过程|损失函数；优化算法；训练技巧与挑战|
|第五章 |使用Numpy实现Transformer|手撕代码|
|第六章   | Transformer模型在NLP中的应用|实践项目：案例分析与实验结果|

## 目录
第一章 Transformer模型背景
- 序列到序列模型的发展
- RNN与LSTM的局限性
- Transformer的提出与影响

第二章 Encoder结构
- 自注意力机制(Self-Attention)
- 多头注意力(Multi-Head Attention)
-  位置编码(Positional Encoding)
-  前馈神经网络(Feed-Forward Neural Network)
-  层归一化(Layer Normalization)

第三章 Decoder结构
-  多头注意力 (Mask-Multi-Head-Attention
- 编码器-解码器注意力(Encoder-Decoder Attention)
- 解码器自注意力(Decoder Self-Attention)
-  解码器输出层(Linear and Softmax to Produce Output Probabilities)

第四章 Transformer的训练过程
- 损失函数
- 优化算法
- 训练技巧与挑战
  
第五章 使用Numpy实现Transformer

第六章 Transformer模型在NLP中的应用

## 参与贡献

- 如果你想参与到项目中来欢迎查看项目的 [Issue]() 查看没有被分配的任务。
- 如果你发现了一些问题，欢迎在 [Issue]() 中进行反馈🐛。
- 如果你对本项目感兴趣想要参与进来可以通过 [Discussion]() 进行交流💬。

如果你对 Datawhale 很感兴趣并想要发起一个新的项目，欢迎查看 [Datawhale 贡献指南](https://github.com/datawhalechina/DOPMC#%E4%B8%BA-datawhale-%E5%81%9A%E5%87%BA%E8%B4%A1%E7%8C%AE)。

## 贡献者名单

| 姓名 | 职责 | 简介 |
| :----| :---- | :---- |
| 罗清泉 | 项目负责人 |  |



## 关注我们

<div align=center>
<p>扫描下方二维码关注公众号：Datawhale</p>
<img src="https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/qrcode.jpeg" width = "180" height = "180">
</div>

## LICENSE

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。

*注：默认使用CC 4.0协议，也可根据自身项目情况选用其他协议*
