{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca112b5",
   "metadata": {},
   "source": [
    "# 多头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a016aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 128])\n",
      "Backward pass completed.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义一个 MultiHeadAttention 类，它继承自 nn.Module\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        # 调用父类的构造函数\n",
    "        super().__init__()\n",
    "        # 保存模型维度和头数\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads  # 每个头对应的维度\n",
    "        self.h = heads  # 头的数量\n",
    "\n",
    "        # 初始化线性层，用于将输入转换为查询（Q）、键（K）和值（V）\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        # 初始化Dropout层，用于正则化\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 初始化输出线性层，用于将多头注意力输出转换为模型维度\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # 定义注意力机制的计算过程\n",
    "    def attention(self, q, k, v, mask=None):\n",
    "        # 计算Q和K的矩阵乘积，然后除以根号下d_k\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # 如果提供了掩码，则将掩码对应的位置设置为负无穷，这样在softmax后这些位置的值为0\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # 应用softmax函数获得注意力权重\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        # 应用dropout\n",
    "        scores = self.dropout(scores)\n",
    "        # 将注意力权重和V相乘得到输出\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "\n",
    "    # 定义前向传播过程\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        # 将输入Q、K、V通过线性层，并调整形状以进行多头注意力计算\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # 计算注意力输出\n",
    "        scores = self.attention(q, k, v, mask)\n",
    "        # 将多头输出合并，并调整形状以匹配模型维度\n",
    "        concat = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        # 通过输出线性层\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "\n",
    "# 主函数，用于测试 MultiHeadAttention 类\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化模型参数\n",
    "    heads = 4\n",
    "    d_model = 128  # d_model应该是heads的整数倍。\n",
    "    dropout = 0.1\n",
    "\n",
    "    # 创建 MultiHeadAttention 实例\n",
    "    model = MultiHeadAttention(heads, d_model, dropout)\n",
    "\n",
    "    # 创建随机数据作为输入\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    q = torch.rand(batch_size, seq_len, d_model)  # Query\n",
    "    k = torch.rand(batch_size, seq_len, d_model)  # Key\n",
    "    v = torch.rand(batch_size, seq_len, d_model)  # Value\n",
    "\n",
    "    # 执行前向传播\n",
    "    output = model(q, k, v)\n",
    "\n",
    "    # 打印输出形状，应该是 [batch_size, seq_len, d_model]\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # 检查模型是否可以进行反向传播\n",
    "    loss = output.mean()  # 创建一个简单的损失函数\n",
    "    loss.backward()  # 执行反向传播\n",
    "    print(\"Backward pass completed.\")  # 如果没有错误，则表示反向传播成功\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12382b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcde6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
